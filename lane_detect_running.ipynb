{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Video ...\n",
      "Detecting Lanes in each frame ...\n",
      "Frame:  1\n",
      "Frame:  2\n",
      "Frame:  3\n",
      "Frame:  4\n",
      "Frame:  5\n",
      "Frame:  6\n",
      "Frame:  7\n",
      "Frame:  8\n",
      "Frame:  9\n",
      "Frame:  10\n",
      "Frame:  11\n",
      "Frame:  12\n",
      "Frame:  13\n",
      "Frame:  14\n",
      "Frame:  15\n",
      "Frame:  16\n",
      "Frame:  17\n",
      "Frame:  18\n",
      "Frame:  19\n",
      "Frame:  20\n",
      "Frame:  21\n",
      "Frame:  22\n",
      "Frame:  23\n",
      "Frame:  24\n",
      "Frame:  25\n",
      "Frame:  26\n",
      "Frame:  27\n",
      "Frame:  28\n",
      "Frame:  29\n",
      "Frame:  30\n",
      "Frame:  31\n",
      "Frame:  32\n",
      "Frame:  33\n",
      "Frame:  34\n",
      "Frame:  35\n",
      "Frame:  36\n",
      "Frame:  37\n",
      "Frame:  38\n",
      "Frame:  39\n",
      "Frame:  40\n",
      "Frame:  41\n",
      "Frame:  42\n",
      "Frame:  43\n",
      "Frame:  44\n",
      "Frame:  45\n",
      "Frame:  46\n",
      "Frame:  47\n",
      "Frame:  48\n",
      "Frame:  49\n",
      "Frame:  50\n",
      "Frame:  51\n",
      "Frame:  52\n",
      "Frame:  53\n",
      "Frame:  54\n",
      "Frame:  55\n",
      "Frame:  56\n",
      "Frame:  57\n",
      "Frame:  58\n",
      "Frame:  59\n",
      "Frame:  60\n",
      "Frame:  61\n",
      "Frame:  62\n",
      "Frame:  63\n",
      "Frame:  64\n",
      "Frame:  65\n",
      "Frame:  66\n",
      "Frame:  67\n",
      "Frame:  68\n",
      "Frame:  69\n",
      "Frame:  70\n",
      "Frame:  71\n",
      "Frame:  72\n",
      "Frame:  73\n",
      "Frame:  74\n",
      "Frame:  75\n",
      "Frame:  76\n",
      "Frame:  77\n",
      "Frame:  78\n",
      "Frame:  79\n",
      "Frame:  80\n",
      "Frame:  81\n",
      "Frame:  82\n",
      "Frame:  83\n",
      "Frame:  84\n",
      "Frame:  85\n",
      "Frame:  86\n",
      "Frame:  87\n",
      "Frame:  88\n",
      "Frame:  89\n",
      "Frame:  90\n",
      "Frame:  91\n",
      "Frame:  92\n",
      "Frame:  93\n",
      "Frame:  94\n",
      "Frame:  95\n",
      "Frame:  96\n",
      "Frame:  97\n",
      "Frame:  98\n",
      "Frame:  99\n",
      "Frame:  100\n",
      "Frame:  101\n",
      "Frame:  102\n",
      "Frame:  103\n",
      "Frame:  104\n",
      "Frame:  105\n",
      "Frame:  106\n",
      "Frame:  107\n",
      "Frame:  108\n",
      "Frame:  109\n",
      "Frame:  110\n",
      "Frame:  111\n",
      "Frame:  112\n",
      "Frame:  113\n",
      "Frame:  114\n",
      "Frame:  115\n",
      "Frame:  116\n",
      "Frame:  117\n",
      "Frame:  118\n",
      "Frame:  119\n",
      "Frame:  120\n",
      "Frame:  121\n",
      "Frame:  122\n",
      "Frame:  123\n",
      "Frame:  124\n",
      "Frame:  125\n",
      "Frame:  126\n",
      "Frame:  127\n",
      "Frame:  128\n",
      "Frame:  129\n",
      "Frame:  130\n",
      "Frame:  131\n",
      "Frame:  132\n",
      "Frame:  133\n",
      "Frame:  134\n",
      "Frame:  135\n",
      "Frame:  136\n",
      "Frame:  137\n",
      "Frame:  138\n",
      "Frame:  139\n",
      "Frame:  140\n",
      "Frame:  141\n",
      "Frame:  142\n",
      "Frame:  143\n",
      "Frame:  144\n",
      "Frame:  145\n",
      "Frame:  146\n",
      "Frame:  147\n",
      "Frame:  148\n",
      "Frame:  149\n",
      "Frame:  150\n",
      "Frame:  151\n",
      "Frame:  152\n",
      "Frame:  153\n",
      "Frame:  154\n",
      "Frame:  155\n",
      "Frame:  156\n",
      "Frame:  157\n",
      "Frame:  158\n",
      "Frame:  159\n",
      "Frame:  160\n",
      "Frame:  161\n",
      "Frame:  162\n",
      "Frame:  163\n",
      "Frame:  164\n",
      "Frame:  165\n",
      "Frame:  166\n",
      "Frame:  167\n",
      "Frame:  168\n",
      "Frame:  169\n",
      "Frame:  170\n",
      "Frame:  171\n",
      "Frame:  172\n",
      "Frame:  173\n",
      "Frame:  174\n",
      "Frame:  175\n",
      "Frame:  176\n",
      "Frame:  177\n",
      "Frame:  178\n",
      "Frame:  179\n",
      "Frame:  180\n",
      "Frame:  181\n",
      "Frame:  182\n",
      "Frame:  183\n",
      "Frame:  184\n",
      "Frame:  185\n",
      "Frame:  186\n",
      "Frame:  187\n",
      "Frame:  188\n",
      "Frame:  189\n",
      "Frame:  190\n",
      "Frame:  191\n",
      "Frame:  192\n",
      "Frame:  193\n",
      "Frame:  194\n",
      "Frame:  195\n",
      "Frame:  196\n",
      "Frame:  197\n",
      "Frame:  198\n",
      "Frame:  199\n",
      "Frame:  200\n",
      "Frame:  201\n",
      "Frame:  202\n",
      "Frame:  203\n",
      "Frame:  204\n",
      "Frame:  205\n",
      "Frame:  206\n",
      "Frame:  207\n",
      "Frame:  208\n",
      "Frame:  209\n",
      "Frame:  210\n",
      "Frame:  211\n",
      "Frame:  212\n",
      "Frame:  213\n",
      "Frame:  214\n",
      "Frame:  215\n",
      "Frame:  216\n",
      "Frame:  217\n",
      "Frame:  218\n",
      "Frame:  219\n",
      "Frame:  220\n",
      "Frame:  221\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: Bharadwaj Chukkala\n",
    "UID: 118341705\n",
    "'''\n",
    "\n",
    "# Importing Necessary Libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "'''\n",
    "Function to define the region of interest\n",
    "''' \n",
    "def region_of_interest(img):\n",
    "    img_shape = img.shape[:2]\n",
    "    vertices = np.array([[(0,img_shape[0]), (9*img_shape[1]/20, 11*img_shape[0]/18), (11*img_shape[1]/20, 11*img_shape[0]/18), (img_shape[1],img_shape[0])]], dtype=np.int32)\n",
    "    mask = np.zeros_like(img).astype(np.uint8)\n",
    "    cv2.fillPoly(mask, [vertices], (255,255,255))\n",
    "    image= cv2.bitwise_and(mask,img)\n",
    "    return image\n",
    "\n",
    "\n",
    "'''\n",
    "Function to extract lane Lines using Hough Transform\n",
    "'''\n",
    "def Hough_transform(image):\n",
    "    \n",
    "    img_shape = image.shape\n",
    "    vertices = np.array([[(0,img_shape[0]), (9*img_shape[1]/20, 11*img_shape[0]/18), (11*img_shape[1]/20, 11*img_shape[0]/18), (img_shape[1],img_shape[0])]], dtype=np.int32)\n",
    "   \n",
    "    # Converting image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Applying a gaussian blur mask on the gray image\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # detecting edges in the image\n",
    "    edges = cv2.Canny(blur, 25, 100)\n",
    "\n",
    "    # Defining the region  of interest\n",
    "    mask = np.zeros_like(edges)\n",
    "    cv2.fillPoly(mask, vertices, 255)\n",
    "    masked = cv2.bitwise_and(edges, mask)\n",
    "    img_r_2=region_of_interest(image)\n",
    "    new_img=cv2.bitwise_and(img_r_2,img_r_2,mask=masked)\n",
    "    new_img=cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Performing Hough transform to detect lanes\n",
    "    lines = cv2.HoughLinesP(new_img, 1, np.pi/180, 14, np.array([]), minLineLength=30, maxLineGap=60)\n",
    "    hough_image = np.zeros((*new_img.shape, 3), dtype=np.uint8)\n",
    "\n",
    "    return hough_image,masked,lines,edges    \n",
    "\n",
    "\n",
    "'''\n",
    "Function to get Histogram of the image\n",
    "'''\n",
    "def show_histogram(image):\n",
    "    histogram=np.sum(image[image.shape[0]//2:,:],axis=0)\n",
    "    return histogram\n",
    "\n",
    "\n",
    "'''    \n",
    "Function differentiates between the solid and dashed lines using Histogram peaks\n",
    "'''\n",
    "def lane_detector(image):\n",
    "    img_hist= show_histogram(image)\n",
    "    midpoint_current=int(img_hist.shape[0]/2)\n",
    "    right_current_x=np.argmax(img_hist[midpoint_current:])+midpoint_current\n",
    "    left_current_x=np.argmax(img_hist[:midpoint_current])\n",
    "    non_zero_pixels=img_hist.nonzero()\n",
    "    mid_reg=int(image.shape[1]/2)\n",
    "    left_part=image[:,:mid_reg]\n",
    "    right_part=image[:,mid_reg:]\n",
    "    left_count = cv2.findNonZero(left_part)\n",
    "    right_count=cv2.findNonZero(right_part)\n",
    " \n",
    "\n",
    "    return left_current_x,right_current_x,non_zero_pixels,left_count,right_count\n",
    "\n",
    "'''\n",
    "Function to show output image\n",
    "'''\n",
    "def resultant(image,hough_image,lines,l_count,r_count):\n",
    "   \n",
    "    if  r_count.shape[0] > l_count.shape[0]:\n",
    "        color_left = (0,0,255)\n",
    "        text1=\"Left Lane: Dashed Lines Detected\"\n",
    "        color_right= (0,255,0)\n",
    "        text2=\"Right Lane: Solid Lines Detected \"\n",
    "    else:\n",
    "         print(\"left\")\n",
    "         color_left = (0,255,0)   \n",
    "         text1 = \"Left Lane: Solid Lines Detected \"\n",
    "         color_right = (0,0,255)    \n",
    "         text2=\"Right Lane: Dashed Lines Detected \" \n",
    "         \n",
    "    # Plotting the left and right lane lines    \n",
    "    # WEIGHTED IMAGE\n",
    "    draw_lines(hough_image,lines,color_left,color_right)\n",
    "    \n",
    "    processed = cv2.addWeighted(image, 0.8, hough_image, 1, 0)\n",
    "    \n",
    "    cv2.putText(processed,text1,(10,100),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),1)\n",
    "    cv2.putText(processed,text2,(10,150),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),1)\n",
    "\n",
    "    \n",
    "    return processed\n",
    "\n",
    "\n",
    "'''\n",
    "Function to draw different colored lines on the detected lane lines\n",
    "'''\n",
    "def draw_lines(img, lines,color_left,color_right,thickness=12):\n",
    "   \n",
    "    global CACHE_LEFT_SLOPE\n",
    "    global CACHE_RIGHT_SLOPE\n",
    "    global CACHE_LEFT\n",
    "    global CACHE_RIGHT\n",
    "\n",
    "    # DECLARE VARIABLES\n",
    "    cache_weight = 0.9\n",
    "\n",
    "    right_ys = []\n",
    "    right_xs = []\n",
    "    right_slopes = []\n",
    "\n",
    "    left_ys = []\n",
    "    left_xs = []\n",
    "    left_slopes = []\n",
    "\n",
    "    midpoint = img.shape[1] / 2\n",
    "\n",
    "    bottom_of_image = img.shape[0]\n",
    "    \n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            slope, yint = np.polyfit((x1, x2), (y1, y2), 1)\n",
    "            # Filter lines using slope and x position\n",
    "            if .35 < np.absolute(slope) <= .85:\n",
    "                if slope > 0 and x1 > midpoint and x2 > midpoint:\n",
    "                    right_ys.append(y1)\n",
    "                    right_ys.append(y2)\n",
    "                    right_xs.append(x1)\n",
    "                    right_xs.append(x2)\n",
    "                    right_slopes.append(slope)\n",
    "                elif slope < 0 and x1 < midpoint and x2 < midpoint:\n",
    "                    left_ys.append(y1)\n",
    "                    left_ys.append(y2)\n",
    "                    left_xs.append(x1)\n",
    "                    left_xs.append(x2)\n",
    "                    left_slopes.append(slope)\n",
    "   \n",
    "    \n",
    "    # DRAW RIGHT LANE LINE\n",
    "    if right_ys:\n",
    "        right_index = right_ys.index(min(right_ys))\n",
    "        right_x1 = right_xs[right_index]\n",
    "        right_y1 = right_ys[right_index]\n",
    "        right_slope = np.median(right_slopes)\n",
    "        if CACHE_RIGHT_SLOPE != 0:\n",
    "            right_slope = right_slope + (CACHE_RIGHT_SLOPE - right_slope) * cache_weight\n",
    "\n",
    "        right_x2 = int(right_x1 + (bottom_of_image - right_y1) / right_slope)\n",
    "\n",
    "        if CACHE_RIGHT_SLOPE != 0:\n",
    "            right_x1 = int(right_x1 + (CACHE_RIGHT[0] - right_x1) * cache_weight)\n",
    "            right_y1 = int(right_y1 + (CACHE_RIGHT[1] - right_y1) * cache_weight)\n",
    "            right_x2 = int(right_x2 + (CACHE_RIGHT[2] - right_x2) * cache_weight)\n",
    "\n",
    "        CACHE_RIGHT_SLOPE = right_slope\n",
    "        CACHE_RIGHT = [right_x1, right_y1, right_x2]\n",
    "        cv2.line(img, (right_x1, right_y1), (right_x2, bottom_of_image), color_right, thickness)\n",
    "        \n",
    "        \n",
    "\n",
    "    # DRAW LEFT LANE LINE\n",
    "    if left_ys:\n",
    "        left_index = left_ys.index(min(left_ys))\n",
    "        left_x1 = left_xs[left_index]\n",
    "        left_y1 = left_ys[left_index]\n",
    "        left_slope = np.median(left_slopes)\n",
    "        if CACHE_LEFT_SLOPE != 0:\n",
    "            left_slope = left_slope + (CACHE_LEFT_SLOPE - left_slope) * cache_weight\n",
    "\n",
    "        left_x2 = int(left_x1 + (bottom_of_image - left_y1) / left_slope)\n",
    "\n",
    "        if CACHE_LEFT_SLOPE != 0:\n",
    "            left_x1 = int(left_x1 + (CACHE_LEFT[0] - left_x1) * cache_weight)\n",
    "            left_y1 = int(left_y1 + (CACHE_LEFT[1] - left_y1) * cache_weight)\n",
    "\n",
    "        CACHE_LEFT_SLOPE = left_slope\n",
    "        CACHE_LEFT = [left_x1, left_y1, left_x2]\n",
    "        cv2.line(img, (left_x1, left_y1), (left_x2, bottom_of_image), color_left, thickness)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def info_display(org_img, edges_img, bird_img, warp_img, final_output):  # , warped):\n",
    "\n",
    "    # Output Window Dimensions\n",
    "    height, width = 1080, 1920\n",
    "\n",
    "    # Output Window\n",
    "    final_img = np.zeros((height, width, 3), np.uint8)\n",
    "\n",
    "    # Creating depth for Edges and Warped Image\n",
    "    edges_img = np.dstack((edges_img, edges_img, edges_img))\n",
    "    bird_img = np.dstack((bird_img, bird_img, bird_img))\n",
    "    warp_img = np.dstack((warp_img, warp_img, warp_img))\n",
    "\n",
    "    # Text\n",
    "    cv2.putText(org_img, '[1] Input Image Frame', (30, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (255, 0, 0), 3, 0)\n",
    "    cv2.putText(edges_img, '[2] Detected Contours', (30, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (255, 0, 0), 3, 0)\n",
    "    cv2.putText(bird_img, '[3] Masked Image', (60, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (255, 0, 0), 3, 0)\n",
    "    cv2.putText(warp_img, '[4] Warpped Image', (10, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 0, 0), 1, 0)\n",
    "\n",
    "    # Predicted Image\n",
    "    final_img[0:620, 640:1920] = cv2.resize(final_output, (1280, 620), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Original Image\n",
    "    final_img[0:620, 0:640] = cv2.resize(org_img, (640, 620), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Edge Image\n",
    "    final_img[620:1080, 0:640] = cv2.resize(edges_img, (640, 460), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Warped Image\n",
    "    final_img[620:1080, 640:1280] = cv2.resize(bird_img, (640, 460), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Sliding Window\n",
    "    final_img[620:1080, 1280:1920] = cv2.resize(warp_img, (640, 460), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    neon = np.zeros((100, final_img.shape[1], 3), np.uint8)\n",
    "    neon[:] = (255, 0, 180)\n",
    "\n",
    "    final_img = cv2.vconcat((final_img, neon))\n",
    "\n",
    "    return final_img\n",
    "\n",
    "# Source points for homography.\n",
    "bird_eye_coords_= np.float32([[410,335], [535, 334], [780, 479], [150, 496]])\n",
    "# bird_eye_coords_=np.float32([[422,321],[540,330],[790,485],[80,500]])\n",
    "# Destination points for homography\n",
    "world_coords_ = np.float32([[50, 0], [250, 0], [250, 500], [0, 500]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source = 'whiteline.mp4'\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    print('Reading Video ...')\n",
    "    print(\"Detecting Lanes in each frame ...\")\n",
    "    output_filename='../bchukkal_proj2/output_videos/lane_detection.mp4'\n",
    "    size=(600,400)\n",
    "    \n",
    "    CACHE_LEFT_SLOPE = 0\n",
    "    CACHE_RIGHT_SLOPE = 0\n",
    "    CACHE_LEFT = [0, 0, 0]\n",
    "    CACHE_RIGHT = [0, 0, 0]\n",
    "    \n",
    "    result = cv2.VideoWriter(output_filename,cv2.VideoWriter_fourcc(*'mp4v'), 15, size)\n",
    "\n",
    "    Frame=0\n",
    "    while(cap.isOpened()):\n",
    "        ret, img = cap.read()\n",
    "        if ret:\n",
    "            Frame+=1\n",
    "            print('Frame: ',Frame) \n",
    "            \n",
    "            # Uncomment the below line to see the video flipped horizontally\n",
    "            #img=cv2.flip(img,1) \n",
    "            \n",
    "            hough_image,masked_edges,lines,edges=Hough_transform(img)\n",
    "            \n",
    "            h_, mask = cv2.findHomography( bird_eye_coords_,world_coords_,cv2.RANSAC,5.0)\n",
    "            \n",
    "            warped = cv2.warpPerspective(masked_edges,h_,(300,600),flags=cv2.INTER_LINEAR)\n",
    "            \n",
    "            \n",
    "            l,r,nxy,lcount,rcount=lane_detector(warped)\n",
    "            \n",
    "            final_output=resultant(img,hough_image,lines,lcount,rcount)\n",
    "            \n",
    "            \n",
    "            collage = info_display(img, edges, masked_edges, warped, final_output)\n",
    "            cv2.imshow('Complete Pipeline', collage)\n",
    "            cv2.imwrite('collage.jpg', collage)\n",
    "\n",
    "            # frame_count = 1\n",
    "            # if frame_count == 1:\n",
    "            #     # print(os.listdir(directory))\n",
    "            #     output1 = 'original_image.jpg'\n",
    "            #     output2 = 'masked_image.jpg'\n",
    "            #     output3 = 'hough_image.jpg'\n",
    "            #     output4 = 'edge_image.jpg'\n",
    "            #     output5 = 'warped_image.jpg'\n",
    "            #     output6 = 'final_output.jpg'\n",
    "            #     output7 = 'flipped_output.jpg'\n",
    "            #     output8 = 'total_outputf.jpg'\n",
    "                \n",
    "            #     cv2.imwrite(output1, img)\n",
    "            #     cv2.imwrite(output2, masked_edges)\n",
    "            #     cv2.imwrite(output3, hough_image)\n",
    "            #     cv2.imwrite(output4, edges)\n",
    "            #     cv2.imwrite(output5, warped)\n",
    "            #     cv2.imwrite(output6, final_output)\n",
    "            #     cv2.imwrite(output7, final_output)\n",
    "            #     cv2.imwrite(output8, collage)\n",
    "            \n",
    "            # frame_count += 1\n",
    "            result.write(collage)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "cap.release()  \n",
    "result.release()      \n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca1530fc43f02cacc5a79445da3769e63c13907fb4d317de69fe7e5797815707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
